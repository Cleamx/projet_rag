# ğŸ¯ Projet RAG GLPI - Assistant IT Helpdesk avec IA

Application web complÃ¨te de RAG (Retrieval-Augmented Generation) pour un helpdesk IT :
- ğŸ¤– **IA locale** : Mistral via Ollama (gÃ©nÃ©ration) + nomic-embed-text (embeddings 768D)
## ğŸš€ Installation rapide avec Docker (recommandÃ©)

### PrÃ©requis
- Docker Desktop installÃ© et lancÃ©
- Au moins 8 GB de RAM disponibles pour Docker

### DÃ©marrage en 3 Ã©tapes

```zsh
# 1. Lancer tous les services
docker-compose up -d

# 2. TÃ©lÃ©charger les modÃ¨les Ollama (premiÃ¨re fois seulement)
docker exec -it ollama_service ollama pull mistral
docker exec -it ollama_service ollama pull nomic-embed-text

# 3. AccÃ©der Ã  l'application
open http://localhost:8000
```

**C'est tout ! L'application est prÃªte.** ğŸ‰

---

## ğŸ“¦ Services Docker

| Service | URL | Description |
|---------|-----|-------------|
| **Frontend** | http://localhost:8000 | Interface chat |
| **API** | http://localhost:8000/docs | Documentation Swagger |
| **PostgreSQL** | localhost:5432 | Base de donnÃ©es (user/password) |
| **Ollama** | http://localhost:11434 | Service LLM local |

---

## ğŸ’» Installation manuelle (sans Docker)

### 1. Installer Ollama
```zsh
# macOS
brew install ollama
ollama serve  # Dans un terminal sÃ©parÃ©

# TÃ©lÃ©charger les modÃ¨les
ollama pull mistral
ollama pull nomic-embed-text
```

### 2. PostgreSQL avec pgvector
```sql
CREATE DATABASE mydatabase;
\c mydatabase
CREATE EXTENSION vector;
```

### 3. Backend Python
```zsh
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Variables d'environnement
export DATABASE_URL="postgresql://user:password@localhost/mydatabase"
export OLLAMA_HOST="http://localhost:11434"

# Lancer
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Le frontend sera accessible sur http://localhost:8000

---

## ğŸ¯ Utilisation

### Interface web
1. Ouvrir http://localhost:8000
2. Entrer un User AD ID (ex: 1)
3. Poser une question IT

### Questions de test
- "Comment me connecter au VPN ?"
- "Mon imprimante ne fonctionne pas"
- "J'ai oubliÃ© mon mot de passe"
- "Outlook est trÃ¨s lent"
- "Comment accÃ©der au dossier partagÃ© ?"

### API REST
```zsh
# Statistiques GLPI
curl http://localhost:8000/glpi/stats

# Poser une question
curl -X POST http://localhost:8000/ask/ \
  -H "Content-Type: application/json" \
  -d '{"user_ad_id": 1, "question": "Comment configurer le VPN ?"}'
```

**RÃ©ponse attendue :**
```json
{
  "question": "Comment configurer le VPN ?",
  "answer": "Pour configurer le VPN, suivez...",
  "sources": [
    {"type": "kb_article", "id": 1, "title": "Configuration VPN - Guide complet"},
    {"type": "faq", "id": 3, "title": "Comment accÃ©der au VPN en tÃ©lÃ©travail ?"}
  ]
}
```

---

## ğŸ“š DonnÃ©es GLPI mockÃ©es

Le systÃ¨me contient des donnÃ©es de dÃ©monstration rÃ©alistes :
- âœ… **8 tickets** IT rÃ©solus (VPN, imprimante, mots de passe, Outlook, etc.)
- âœ… **3 articles** de base de connaissances (guides dÃ©taillÃ©s)
- âœ… **5 items FAQ** (questions frÃ©quentes)

Fichier : `backend/app/glpi_mock.py`

### Endpoints disponibles
```zsh
# Stats globales
curl http://localhost:8000/glpi/stats

# AperÃ§u par type
curl http://localhost:8000/glpi/preview/tickets
curl http://localhost:8000/glpi/preview/kb_articles
curl http://localhost:8000/glpi/preview/faq
```

---

## ğŸ—ï¸ Architecture RAG

```
Question utilisateur
       â†“
   Recherche GLPI mock
   (scoring par mots-clÃ©s)
       â†“
   Top 4 sources pertinentes
       â†“
   Contexte + Question â†’ Mistral
       â†“
   RÃ©ponse + Sources
       â†“
   Sauvegarde PostgreSQL (avec embedding)
```

### Composants
1. **Frontend** : HTML/CSS/JS Ã©purÃ©, chat synchrone
2. **Backend** : FastAPI avec endpoints `/ask/` et `/glpi/*`
3. **RAG** : Recherche dans donnÃ©es GLPI + gÃ©nÃ©ration Mistral
4. **Base** : PostgreSQL avec pgvector (embeddings 768D)
5. **LLM** : Ollama local (mistral + nomic-embed-text)

---

## ğŸ”§ Commandes Docker utiles

```zsh
# Voir les logs
docker-compose logs -f api      # Backend
docker-compose logs -f ollama   # Ollama
docker-compose logs -f db       # PostgreSQL

# RedÃ©marrer un service
docker-compose restart api

# ArrÃªter tout
docker-compose down

# Tout supprimer (y compris volumes)
docker-compose down -v

# Reconstruire aprÃ¨s modification
docker-compose up --build

# AccÃ©der Ã  un conteneur
docker exec -it fastapi_api bash
docker exec -it postgres_db psql -U user -d mydatabase
docker exec -it ollama_service ollama list
```

---

## ğŸ§ª Tests

### Script de test automatisÃ©
```zsh
python3 test_rag.py
```

### Tests manuels
```zsh
# 1. VÃ©rifier PostgreSQL
docker exec -it postgres_db psql -U user -d mydatabase -c "SELECT version();"

# 2. VÃ©rifier Ollama
docker exec -it ollama_service ollama list

# 3. Tester l'API
curl http://localhost:8000/glpi/stats

# 4. Tester une question
curl -X POST http://localhost:8000/ask/ \
  -H "Content-Type: application/json" \
  -d '{"user_ad_id": 1, "question": "Comment configurer le VPN ?"}'
```

---

## ğŸ“ Structure du projet

```
projet_rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # API FastAPI
â”‚   â”‚   â”œâ”€â”€ llm.py            # IntÃ©gration Ollama + RAG
â”‚   â”‚   â”œâ”€â”€ database.py       # PostgreSQL
â”‚   â”‚   â”œâ”€â”€ models.py         # SQLModel (Question, Reponse)
â”‚   â”‚   â””â”€â”€ glpi_mock.py      # DonnÃ©es GLPI mockÃ©es
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ test_rag.py               # Tests automatisÃ©s
â””â”€â”€ README.md                 # Ce fichier
```

---

## ğŸ”„ Migration vers GLPI rÃ©el

Actuellement, les donnÃ©es sont mockÃ©es. Pour connecter une vraie instance GLPI :

### 1. CrÃ©er le connecteur
```python
# backend/app/glpi_connector.py
import requests

class GLPIConnector:
    def __init__(self, api_url, app_token, user_token):
        self.api_url = api_url
        self.headers = {
            "App-Token": app_token,
            "Session-Token": user_token
        }
    
    def search_tickets(self, query):
        # Utiliser l'API REST GLPI
        # GET /search/Ticket
        pass
```

### 2. Remplacer dans llm.py
```python
# Avant
from .glpi_mock import glpi_mock

# AprÃ¨s
from .glpi_connector import GLPIConnector
glpi = GLPIConnector(api_url, app_token, user_token)
```

### 3. Configuration
Ajouter dans `docker-compose.yml` :
```yaml
environment:
  - GLPI_API_URL=https://your-glpi.com/apirest.php
  - GLPI_APP_TOKEN=your_app_token
  - GLPI_USER_TOKEN=your_user_token
```

**Documentation API GLPI** : https://github.com/glpi-project/glpi/blob/main/apirest.md

---

## ğŸ› DÃ©pannage

### Docker ne dÃ©marre pas
```zsh
# VÃ©rifier que Docker Desktop est lancÃ©
docker ps

# Nettoyer et redÃ©marrer
docker-compose down -v
docker-compose up --build
```

### Erreur "expected 384 dimensions, not 768"
```zsh
# RecrÃ©er la base avec les bonnes dimensions
docker-compose down -v
docker-compose up
```

### Frontend ne s'affiche pas
Le frontend est servi par FastAPI. VÃ©rifier :
```zsh
# Logs du backend
docker-compose logs api

# Le dossier frontend est bien montÃ© ?
docker exec -it fastapi_api ls /app/frontend
```

### Ollama n'a pas les modÃ¨les
```zsh
docker exec -it ollama_service ollama pull mistral
docker exec -it ollama_service ollama pull nomic-embed-text
```

### API lente ou timeout
PremiÃ¨re requÃªte plus lente (chargement modÃ¨le). Ensuite normal. Ollama garde les modÃ¨les en cache.

---

## ğŸš€ AmÃ©liorations possibles

1. **Embeddings vectoriels** : Remplacer le scoring par mots-clÃ©s par une vraie recherche vectorielle
2. **Streaming** : Ajouter le streaming des rÃ©ponses pour une meilleure UX
3. **API GLPI rÃ©elle** : Connecter Ã  une vraie instance GLPI
4. **Recherche sÃ©mantique** : Utiliser pgvector pour chercher dans l'historique
5. **Interface admin** : Dashboard pour visualiser les donnÃ©es GLPI
6. **Feedback** : SystÃ¨me de notation des rÃ©ponses pour amÃ©liorer le modÃ¨le
7. **Multi-langues** : Support anglais/franÃ§ais
8. **Auth** : Authentification utilisateur SSO/LDAP

---

## ğŸ“ Variables d'environnement

### Docker (docker-compose.yml)
```yaml
DATABASE_URL: postgresql://user:password@db/mydatabase
OLLAMA_HOST: http://ollama:11434
```

### Manuel (backend/.env)
```bash
DATABASE_URL=postgresql://user:password@localhost/mydatabase
OLLAMA_HOST=http://localhost:11434
# Optionnel pour GLPI rÃ©el :
# GLPI_API_URL=https://your-glpi.com/apirest.php
# GLPI_APP_TOKEN=...
# GLPI_USER_TOKEN=...
```

---

## ğŸ“„ Licence & Contact

Projet acadÃ©mique M2 - 2025

**Technologies :**
- FastAPI 0.119.0
- PostgreSQL 16 + pgvector 0.4.1
- Ollama 0.13.0 (Mistral + nomic-embed-text)
- SQLModel 0.0.27
- Docker Compose

---

## âœ¨ RÃ©sumÃ© rapide

**DÃ©marrer l'application :**
```zsh
docker-compose up -d
docker exec -it ollama_service ollama pull mistral
docker exec -it ollama_service ollama pull nomic-embed-text
open http://localhost:8000
```

**Tester :**
```zsh
curl http://localhost:8000/glpi/stats
curl -X POST http://localhost:8000/ask/ \
  -H "Content-Type: application/json" \
  -d '{"user_ad_id": 1, "question": "Comment configurer le VPN ?"}'
```

**Stopper :**
```zsh
docker-compose down
```

VoilÃ  ! Vous avez un assistant IT helpdesk intelligent avec RAG fonctionnel ! ğŸ‰ d'Ã©chec).

## DÃ©pannage rapide
- VÃ©rifier que `MISTRAL_API_KEY` est valide.
- Activer les logs (si disponible) pour tracer les requÃªtes.
- S'assurer que la branche locale est Ã  jour: `git pull --rebase origin test_api_mistral`
