"""Module d'int√©gration avec Ollama pour LLM et embeddings."""
from typing import Dict, List, Tuple

import ollama

from .glpi_mock import glpi_mock

client = ollama.Client(host="http://ollama:11434")
MODEL_NAME = "mistral"
EMBEDDING_MODEL_NAME = "nomic-embed-text"


def get_embedding(text: str) -> list[float]:
    """G√©n√®re un embedding vectoriel pour le texte donn√©.

    Args:
        text: Texte √† vectoriser

    Returns:
        Liste de floats repr√©sentant l'embedding
    """
    response = client.embeddings(model=EMBEDDING_MODEL_NAME, prompt=text)
    return response["embedding"]


def get_chat_response(question: str) -> str:
    """Obtient une r√©ponse directe du LLM.

    Args:
        question: Question √† poser au mod√®le

    Returns:
        R√©ponse g√©n√©r√©e par le mod√®le
    """
    response = client.chat(
        model=MODEL_NAME, messages=[{"role": "user", "content": question}]
    )
    return response["message"]["content"]


def get_rag_response(
    question: str, top_k: int = 4
) -> Tuple[str, List[Dict]]:
    """G√©n√®re une r√©ponse en utilisant RAG avec similarit√© vectorielle pgvector.

    Args:
        question: Question de l'utilisateur
        top_k: Nombre de sources √† r√©cup√©rer

    Returns:
        Tuple (r√©ponse_g√©n√©r√©e, sources_utilis√©es)
    """
    
    # 1. G√©n√©rer l'embedding de la question
    print(f"üîç G√©n√©ration de l'embedding pour: {question[:50]}...")
    question_embedding = get_embedding(question)
    
    # 2. Chercher dans PostgreSQL avec pgvector (solutions des techniciens)
    from .database import engine
    from sqlmodel import Session, text
    
    rag_results = []
    
    with Session(engine) as session:
        # Convertir embedding en format PostgreSQL vector
        embedding_str = "[" + ",".join(str(x) for x in question_embedding) + "]"
        
        # Requ√™te SQL avec pgvector pour similarit√© cosine
        query = text("""
            SELECT 
                q.id,
                q.question_label,
                r.reponse_label,
                (q.embedding_question <=> :embedding::vector) AS distance
            FROM question q
            JOIN reponse r ON r.question_id = q.id
            WHERE r.reponse_label IS NOT NULL
            ORDER BY distance ASC
            LIMIT :limit
        """)
        
        try:
            results = session.execute(
                query,
                {"embedding": embedding_str, "limit": top_k}
            ).fetchall()
            
            print(f"‚úÖ Recherche vectorielle: {len(results)} r√©sultats trouv√©s dans le RAG")
            
            # Convertir les r√©sultats en format exploitable
            for q_id, question_label, reponse_label, distance in results:
                similarity = 1 - float(distance)  # Convertir distance en similarit√©
                print(f"   - Question #{q_id}: similarit√© = {similarity:.2f}")
                
                rag_results.append({
                    "source": "question",
                    "id": q_id,
                    "title": question_label[:100],
                    "content": f"**Probl√®me**: {question_label}\n\n**Solution**: {reponse_label}",
                    "metadata": {
                        "distance": float(distance),
                        "similarity": similarity
                    },
                    "score": similarity
                })
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche vectorielle: {e}")
            # Continue sans les r√©sultats RAG
    
    # 3. Ajouter aussi les r√©sultats de glpi_mock (donn√©es statiques)
    glpi_results = glpi_mock.search_all(question, limit=2)
    print(f"üìö Recherche glpi_mock: {len(glpi_results)} r√©sultats trouv√©s")
    
    # 4. Combiner tous les r√©sultats
    all_results = rag_results + glpi_results
    
    # 5. Trier par score (similarit√©) et limiter
    all_results.sort(key=lambda x: x.get("score", 0), reverse=True)
    all_results = all_results[:top_k]
    
    print(f"üéØ Total: {len(all_results)} sources retenues")
    
    # 6. V√©rifier la pertinence des r√©sultats
    SIMILARITY_THRESHOLD = 0.60  # Seuil minimum de 60%

    if not all_results:
        print("‚ö†Ô∏è Aucune source trouv√©e")
        return get_chat_response(question), []  # ‚Üê

    # V√©rifier la similarit√© de la meilleure source
    best_score = all_results[0].get("score", 0)
    print(f"üìä Meilleure similarit√© trouv√©e: {best_score:.0%}")

    if best_score < SIMILARITY_THRESHOLD:
        print(f"‚ö†Ô∏è Similarit√© trop faible ({best_score:.0%} < {SIMILARITY_THRESHOLD:.0%})")
        print("   ‚Üí Aucune bonne r√©ponse disponible")
        # Retourner vide pour d√©clencher la cr√©ation de ticket
        return "", []  # ‚Üê IMPORTANT : sources vides = ticket cr√©√© !
    
    # 7. Construire le contexte √† partir des r√©sultats
    context_parts = []
    sources = []
    
    for i, result in enumerate(all_results, 1):
        source_type = result['source'].upper()
        if result['source'] == 'question':
            source_type = "BASE RAG (Solution technicien)"
        
        context_parts.append(f"[Source {i} - {source_type}]")
        context_parts.append(f"Titre: {result['title']}")
        context_parts.append(result["content"])
        
        # Ajouter la similarit√© si disponible
        if "similarity" in result.get("metadata", {}):
            similarity = result["metadata"]["similarity"]
            context_parts.append(f"Pertinence: {similarity:.0%}")
        
        context_parts.append("\n---\n")
        
        sources.append({
            "type": result["source"],
            "id": result["id"],
            "title": result["title"],
            "metadata": result.get("metadata", {}),
        })
    
    context = "\n".join(context_parts)
    
    # 8. G√©n√©rer la r√©ponse avec Mistral
    prompt = f"""Tu es un assistant IT helpdesk. R√©ponds √† la question en \
utilisant UNIQUEMENT les informations fournies dans le contexte ci-dessous. \
Si l'information n'est pas dans le contexte, dis-le clairement.

CONTEXTE:
{context}

QUESTION: {question}

R√âPONSE (sois concis et pr√©cis, cite les sources si pertinent):"""
    
    print("ü§ñ G√©n√©ration de la r√©ponse avec Mistral...")
    response = client.chat(
        model=MODEL_NAME, messages=[{"role": "user", "content": prompt}]
    )
    
    print("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s")
    
    return response["message"]["content"], sources