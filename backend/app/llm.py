"""Module d'int√©gration avec Ollama pour LLM et embeddings."""
import re
from typing import Dict, List, Optional, Tuple

import ollama

from .glpi_mock import glpi_mock

client = ollama.Client(host="http://ollama:11434")
MODEL_NAME = "mistral"
EMBEDDING_MODEL_NAME = "nomic-embed-text"

# Cat√©gories de techniciens disponibles pour la classification
TECHNICIEN_CATEGORIES = {
    "Techniciens": "Support technique g√©n√©ral, assistance informatique DSIN",
    "R√©seau": "Probl√®mes r√©seau, connexion internet, wifi, c√¢blage",
    "M√©tier": "Applications m√©tier, logiciels sp√©cifiques aux services",
    "SharePoint": "SharePoint, sites collaboratifs, espaces de travail",
    "Exchange": "Exchange, Outlook, messagerie, calendrier, mail",
    "Campus num√©rique": "Services num√©riques du campus, outils p√©dagogiques",
    "Comptes": "Gestion des comptes utilisateurs, mots de passe, acc√®s",
    "Cours en ligne": "Plateforme de cours en ligne, Moodle, e-learning",
    "Audiovisuel": "√âquipements audiovisuels, visioconf√©rence, projecteurs",
    "Copieurs": "Imprimantes, copieurs, photocopieuses Xerox",
    "Suivi de commande": "Suivi des commandes de mat√©riel informatique",
}


def get_embedding(text: str) -> list[float]:
    """G√©n√®re un embedding vectoriel pour le texte donn√©.

    Args:
        text: Texte √† vectoriser

    Returns:
        Liste de floats repr√©sentant l'embedding
    """
    response = client.embeddings(model=EMBEDDING_MODEL_NAME, prompt=text)
    return response["embedding"]


def get_chat_response(question: str) -> str:
    """Obtient une r√©ponse directe du LLM.

    Args:
        question: Question √† poser au mod√®le

    Returns:
        R√©ponse g√©n√©r√©e par le mod√®le
    """
    response = client.chat(
        model=MODEL_NAME, messages=[{"role": "user", "content": question}]
    )
    return response["message"]["content"]


def parse_category_from_response(response: str) -> Tuple[str, Optional[str]]:
    """Parse et extrait le tag de cat√©gorie de la r√©ponse LLM.

    Args:
        response: R√©ponse brute du LLM contenant potentiellement [CATEGORY:xxx]

    Returns:
        Tuple (r√©ponse_nettoy√©e, cat√©gorie) o√π cat√©gorie peut √™tre None
    """
    # Regex plus souple pour capturer [CATEGORY:xxx] ou [xxx] √† la fin
    # On cherche un tag entre crochets contenant une des cat√©gories connues
    pattern = r'\[(?:CATEGORY:)?\s*([^\]]+)\s*\]'
    match = re.search(pattern, response)

    if match:
        possible_category = match.group(1).strip()
        # Nettoyage de la r√©ponse en enlevant tout ce qui ressemble au tag √† la fin
        cleaned_response = re.sub(pattern, '', response).strip()
        
        # V√©rification si le contenu du tag correspond √† une cat√©gorie connue (case insensitive)
        for cat_name in TECHNICIEN_CATEGORIES:
            if cat_name.lower() in possible_category.lower():
                return cleaned_response, cat_name

        return cleaned_response, None

    return response.strip(), None


def _build_categories_prompt() -> str:
    """Construit la partie du prompt listant les cat√©gories disponibles."""
    lines = ["CAT√âGORIES DE TECHNICIENS DISPONIBLES:"]
    for nom, description in TECHNICIEN_CATEGORIES.items():
        lines.append(f"- {nom}: {description}")
    return "\n".join(lines)


def get_rag_response(
    question: str, top_k: int = 4
) -> Tuple[str, List[Dict]]:
    """G√©n√®re une r√©ponse en utilisant RAG avec similarit√© vectorielle pgvector.

    Args:
        question: Question de l'utilisateur
        top_k: Nombre de sources √† r√©cup√©rer

    Returns:
        Tuple (r√©ponse_g√©n√©r√©e, sources_utilis√©es, cat√©gorie_technicien)
    """
    
    # 1. G√©n√©rer l'embedding de la question
    print(f"üîç G√©n√©ration de l'embedding pour: {question[:50]}...")
    question_embedding = get_embedding(question)
    
    # 2. Chercher dans PostgreSQL avec pgvector (solutions des techniciens)
    from .database import engine
    from sqlmodel import Session, text
    
    rag_results = []
    
    with Session(engine) as session:
        # Convertir embedding en format PostgreSQL vector
        embedding_str = "[" + ",".join(str(x) for x in question_embedding) + "]"
        
        # Requ√™te SQL avec pgvector pour similarit√© cosine
        query = text("""
            SELECT 
                q.id,
                q.question_label,
                r.reponse_label,
                (q.embedding_question <=> :embedding::vector) AS distance
            FROM question q
            JOIN reponse r ON r.question_id = q.id
            WHERE r.reponse_label IS NOT NULL
            ORDER BY distance ASC
            LIMIT :limit
        """)
        
        try:
            results = session.execute(
                query,
                {"embedding": embedding_str, "limit": top_k}
            ).fetchall()
            
            print(f"‚úÖ Recherche vectorielle: {len(results)} r√©sultats trouv√©s dans le RAG")
            
            # Convertir les r√©sultats en format exploitable
            for q_id, question_label, reponse_label, distance in results:
                similarity = 1 - float(distance)  # Convertir distance en similarit√©
                print(f"   - Question #{q_id}: similarit√© = {similarity:.2f}")
                
                rag_results.append({
                    "source": "question",
                    "id": q_id,
                    "title": question_label[:100],
                    "content": f"**Probl√®me**: {question_label}\n\n**Solution**: {reponse_label}",
                    "metadata": {
                        "distance": float(distance),
                        "similarity": similarity
                    },
                    "score": similarity
                })
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche vectorielle: {e}")
            # Continue sans les r√©sultats RAG
    
    # 3. Ajouter aussi les r√©sultats de glpi_mock (donn√©es statiques)
    glpi_results = glpi_mock.search_all(question, limit=2)
    print(f"üìö Recherche glpi_mock: {len(glpi_results)} r√©sultats trouv√©s")
    
    # 4. Combiner tous les r√©sultats
    all_results = rag_results + glpi_results
    
    # 5. Trier par score (similarit√©) et limiter
    all_results.sort(key=lambda x: x.get("score", 0), reverse=True)
    all_results = all_results[:top_k]
    
    print(f"üéØ Total: {len(all_results)} sources retenues")
    
    # 6. V√©rifier la pertinence des r√©sultats
    SIMILARITY_THRESHOLD = 0.60  # Seuil minimum de 60%

    if not all_results:
        print("‚ö†Ô∏è Aucune source trouv√©e")
        return get_chat_response(question), []  # ‚Üê

    # V√©rifier la similarit√© de la meilleure source
    best_score = all_results[0].get("score", 0)
    print(f"üìä Meilleure similarit√© trouv√©e: {best_score:.0%}")

    if best_score < SIMILARITY_THRESHOLD:
        print(f"‚ö†Ô∏è Similarit√© trop faible ({best_score:.0%} < {SIMILARITY_THRESHOLD:.0%})")
        print("   ‚Üí Aucune bonne r√©ponse disponible")
        # Retourner vide pour d√©clencher la cr√©ation de ticket
        return "", []  # ‚Üê IMPORTANT : sources vides = ticket cr√©√© !
    
    # 7. Construire le contexte √† partir des r√©sultats
    context_parts = []
    sources = []
    
    for i, result in enumerate(all_results, 1):
        source_type = result['source'].upper()
        if result['source'] == 'question':
            source_type = "BASE RAG (Solution technicien)"
        
        context_parts.append(f"[Source {i} - {source_type}]")
        context_parts.append(f"Titre: {result['title']}")
        context_parts.append(result["content"])
        
        # Ajouter la similarit√© si disponible
        if "similarity" in result.get("metadata", {}):
            similarity = result["metadata"]["similarity"]
            context_parts.append(f"Pertinence: {similarity:.0%}")
        
        context_parts.append("\n---\n")
        
        sources.append({
            "type": result["source"],
            "id": result["id"],
            "title": result["title"],
            "metadata": result.get("metadata", {}),
        })
    
    context = "\n".join(context_parts)
    
    # 8. G√©n√©rer la r√©ponse avec Mistral
    categories_prompt = _build_categories_prompt()

    prompt = f"""Tu es un assistant IT helpdesk. R√©ponds √† la question en \
utilisant UNIQUEMENT les informations fournies dans le contexte ci-dessous. \
Si l'information n'est pas dans le contexte, dis-le clairement.

CONTEXTE:
{context}

{categories_prompt}

QUESTION: {question}

R√âPONSE (sois concis et pr√©cis, cite les sources si pertinent):"""
    
    print("ü§ñ G√©n√©ration de la r√©ponse avec Mistral...")
    response = client.chat(
        model=MODEL_NAME, messages=[{"role": "user", "content": prompt}]
    )
    
    print("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s")
    
    return response["message"]["content"], sources
INSTRUCTIONS:
1. R√©ponds de mani√®re concise et pr√©cise, cite les sources si pertinent.
2. √Ä LA FIN de ta r√©ponse, ajoute un tag [CATEGORY:NomCat√©gorie] pour \
indiquer quel technicien devrait traiter cette question. Choisis la \
cat√©gorie la plus appropri√©e parmi celles list√©es ci-dessus.

R√âPONSE:"""

    response = client.chat(
        model=MODEL_NAME, messages=[{"role": "user", "content": prompt}]
    )

    raw_response = response["message"]["content"]
    cleaned_response, category = parse_category_from_response(raw_response)

    return cleaned_response, sources, category

